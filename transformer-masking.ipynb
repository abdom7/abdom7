{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2727b50c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-07T00:01:50.228249Z",
     "iopub.status.busy": "2024-11-07T00:01:50.227609Z",
     "iopub.status.idle": "2024-11-07T00:02:06.082090Z",
     "shell.execute_reply": "2024-11-07T00:02:06.080801Z"
    },
    "papermill": {
     "duration": 15.864626,
     "end_time": "2024-11-07T00:02:06.085112",
     "exception": false,
     "start_time": "2024-11-07T00:01:50.220486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df5759",
   "metadata": {
    "papermill": {
     "duration": 0.004061,
     "end_time": "2024-11-07T00:02:06.093767",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.089706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\r\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. \r\n",
    "\n",
    "### 1.1 - Padding Mask\r\n",
    "\r\n",
    "Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five, it is fed the following sequences:\r\n",
    "\r\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \r\n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\r\n",
    "     [\"Exciting\", \"!\"]\r\n",
    "    ]\r\n",
    "\r\n",
    "which might get vectorized as:\r\n",
    "\r\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\r\n",
    "     [ 56, 1285, 15, 181, 545],\r\n",
    "     [ 87, 600]\r\n",
    "    ]\r\n",
    "    \r\n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\r\n",
    "\r\n",
    "    [[ 71, 121, 4, 56, 99],\r\n",
    "     [ 2344, 345, 1284, 15, 0],\r\n",
    "     [ 56, 1285, 15, 181, 545],\r\n",
    "     [ 87, 600, 0, 0, 0],\r\n",
    "    ]\r\n",
    "    \r\n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, zeros will also be added for padding.\r\n",
    "\r\n",
    "When pasing these vectors through the attention layers, the zeros will typically disappear  (you will get completely new vectors given the mathematical operations that happen in the attention block). However, you still want the network to attend only to the first few numbers in that vector (given by the sentence length) and this is when a padding mask comes in handy. You will need to define a boolean mask that specifies to which elements you must attend (1) and which elements you must ignore (0) and you do this by looking at all the zeros in the sequence. Then you use the mask to set the values of the vectors (corresponding to the zeros in the initial vector) close to negative infinity (-1e9).\r\n",
    "\r\n",
    "Imagine your input vector is `[87, 600, 0, 0, 0]`. This would give you a mask of `[1, 1, 0, 0, 0]`. When your vector passes through the attention mechanism, you get another (randomly looking) vector, let's say `[1, 2, 3, 4, 5]`, which after masking becomes `[1, 2, -1e9, -1e9, -1e9]`, so that when you take the softmax, the last three elements (where there were zeros in theask of an _already padded sequence_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdc1676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T00:02:06.104302Z",
     "iopub.status.busy": "2024-11-07T00:02:06.103519Z",
     "iopub.status.idle": "2024-11-07T00:02:06.110745Z",
     "shell.execute_reply": "2024-11-07T00:02:06.109390Z"
    },
    "papermill": {
     "duration": 0.015846,
     "end_time": "2024-11-07T00:02:06.113714",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.097868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    mask = tf.math.equal(decoder_token_ids , 0 )\n",
    "    mask = tf.cast(mask , dtype = tf.float32)\n",
    "    seq = 1 - mask \n",
    "    return seq[: ,tf.newaxis , :  ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed021043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T00:02:06.124027Z",
     "iopub.status.busy": "2024-11-07T00:02:06.123584Z",
     "iopub.status.idle": "2024-11-07T00:02:06.159675Z",
     "shell.execute_reply": "2024-11-07T00:02:06.158371Z"
    },
    "papermill": {
     "duration": 0.044348,
     "end_time": "2024-11-07T00:02:06.162408",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.118060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[7., 6., 0., 0., 0.], [1., 2., 3., 0., 0.], [3., 0., 0., 0., 0.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb46a870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T00:02:06.172827Z",
     "iopub.status.busy": "2024-11-07T00:02:06.172404Z",
     "iopub.status.idle": "2024-11-07T00:02:06.280895Z",
     "shell.execute_reply": "2024-11-07T00:02:06.279713Z"
    },
    "papermill": {
     "duration": 0.116748,
     "end_time": "2024-11-07T00:02:06.283542",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.166794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax of non-masked vectors:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[7.2959954e-01 2.6840466e-01 6.6530867e-04 6.6530867e-04 6.6530867e-04]]\n",
      "\n",
      " [[8.4437378e-02 2.2952460e-01 6.2391251e-01 3.1062774e-02 3.1062774e-02]]\n",
      "\n",
      " [[8.3392531e-01 4.1518696e-02 4.1518696e-02 4.1518696e-02 4.1518696e-02]]], shape=(3, 1, 5), dtype=float32)\n",
      "\n",
      "Softmax of masked vectors:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[0.7310586  0.26894143 0.         0.         0.        ]]\n",
      "\n",
      " [[0.09003057 0.24472848 0.66524094 0.         0.        ]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.        ]]], shape=(3, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = create_padding_mask(x)\n",
    "\n",
    "x_extended = x[:, tf.newaxis, :]\n",
    "\n",
    "print(\"Softmax of non-masked vectors:\\n\")\n",
    "print(tf.keras.activations.softmax(x_extended))\n",
    "\n",
    "print(\"\\nSoftmax of masked vectors:\\n\")\n",
    "print(tf.keras.activations.softmax(x_extended + (1 - mask) * -1.0e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724f994",
   "metadata": {
    "papermill": {
     "duration": 0.003813,
     "end_time": "2024-11-07T00:02:06.291579",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.287766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Look-Ahead Mask \n",
    "\r\n",
    "The look-ahead mask follows similar intuition. In training, you will have access to the complete correct output of your training example. The look-ahead mask helps your model pretend that it correctly predicted a part of the output and see if, *without looking ahead*, it can correctly predict the next output. \r\n",
    "\r\n",
    "For example, if the expected correct output is `[1, 2, 3]` and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c027c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T00:02:06.302352Z",
     "iopub.status.busy": "2024-11-07T00:02:06.301164Z",
     "iopub.status.idle": "2024-11-07T00:02:06.307673Z",
     "shell.execute_reply": "2024-11-07T00:02:06.306253Z"
    },
    "papermill": {
     "duration": 0.014708,
     "end_time": "2024-11-07T00:02:06.310356",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.295648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length):\n",
    "    mask = tf.linalg.band_part(tf.ones((1 , sequence_length , sequence_length)) , -1 , 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a763fd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T00:02:06.321066Z",
     "iopub.status.busy": "2024-11-07T00:02:06.320578Z",
     "iopub.status.idle": "2024-11-07T00:02:06.338354Z",
     "shell.execute_reply": "2024-11-07T00:02:06.336983Z"
    },
    "papermill": {
     "duration": 0.026384,
     "end_time": "2024-11-07T00:02:06.340999",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.314615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84698758",
   "metadata": {
    "papermill": {
     "duration": 0.003867,
     "end_time": "2024-11-07T00:02:06.349169",
     "exception": false,
     "start_time": "2024-11-07T00:02:06.345302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.520449,
   "end_time": "2024-11-07T00:02:08.707358",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-07T00:01:47.186909",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
